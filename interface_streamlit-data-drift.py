import streamlit as st
import pandas as pd
import numpy as np
import mlflow
import json
import logging
import os
import streamlit.components.v1 as components # Importation pour st.components.v1.html

# # Imports Evidently - Ces lignes ne sont plus n√©cessaires directement dans app.py
# from evidently.report import Report
# from evidently.metric_preset import DataDriftPreset

# --- 1. Configuration de la Page Streamlit (DOIT ABSOLUMENT √äTRE LA PREMI√àRE COMMANDE ST.) ---
st.set_page_config(
    page_title="Pr√©diction de D√©faut Client & Surveillance",
    page_icon="üìä",
    layout="wide"
)

# --- 2. Configuration du Logging pour Streamlit ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- 3. Configuration MLflow pour l'Application ---
MLFLOW_TRACKING_URI = "http://127.0.0.1:5000"
MLFLOW_MODEL_NAME = "HomeCreditLogisticRegressionPipeline"

try:
    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)
    logger.info(f"Streamlit: MLflow tracking URI set to: {MLFLOW_TRACKING_URI}")
except Exception as e:
    st.error(f"Streamlit: Erreur critique lors de la configuration de MLflow Tracking URI: {e}")
    st.info("V√©rifiez que votre serveur MLflow est d√©marr√© et accessible √† l'adresse sp√©cifi√©e.")
    st.stop()

# --- 4. Fonction de Chargement du Mod√®le et des M√©tadonn√©es (Mise en Cache) ---
@st.cache_resource(show_spinner="Chargement du mod√®le de pr√©diction et de ses m√©tadonn√©es depuis MLflow...")
def load_mlflow_model_and_metadata():
    """
    Charge le mod√®le et ses m√©tadonn√©es (features_info, seuil optimal, all_training_features)
    depuis MLflow Model Registry. Cette fonction est mise en cache pour n'√™tre ex√©cut√©e qu'une seule fois.
    """
    try:
        client = mlflow.tracking.MlflowClient()
        model_versions = client.search_model_versions(f"name='{MLFLOW_MODEL_NAME}'")

        if not model_versions:
            error_msg = (f"Aucune version du mod√®le '{MLFLOW_MODEL_NAME}' trouv√©e dans le Model Registry MLflow. "
                         "Assurez-vous que le script d'entra√Ænement a √©t√© ex√©cut√© et que le mod√®le "
                         "a √©t√© enregistr√© avec le nom sp√©cifi√©.")
            logger.error(f"Streamlit: {error_msg}")
            raise ValueError(error_msg)

        latest_version = max(model_versions, key=lambda mv: mv.version)
        model_uri = f"models:/{MLFLOW_MODEL_NAME}/{latest_version.version}"

        model = mlflow.sklearn.load_model(model_uri=model_uri)
        logger.info(f"Streamlit: Mod√®le '{MLFLOW_MODEL_NAME}' (Version {latest_version.version}) charg√© avec succ√®s en tant que Scikit-learn model.")

        features_info = None
        optimal_threshold = None
        all_training_features = None

        # Tenter de r√©cup√©rer des tags (pr√©f√©rable)
        model_metadata_tags = latest_version.tags if hasattr(latest_version, 'tags') else {}
        optimal_threshold_str = model_metadata_tags.get('mlflow.log_model.metadata.optimal_threshold')
        features_info_str = model_metadata_tags.get('mlflow.log_model.metadata.features_info_for_streamlit')
        all_training_features_str = model_metadata_tags.get('mlflow.log_model.metadata.all_training_features')

        if optimal_threshold_str:
            optimal_threshold = float(optimal_threshold_str)
            logger.info(f"Streamlit: Seuil optimal r√©cup√©r√© des tags du mod√®le: {optimal_threshold:.4f}")
        else:
            logger.warning("Streamlit: Seuil optimal non trouv√© dans les tags du mod√®le. Tentative de fallback...")

        if features_info_str:
            features_info = json.loads(features_info_str)
            logger.info(f"Streamlit: Informations sur les features r√©cup√©r√©es des tags du mod√®le ({len(features_info)} features).")
        else:
            logger.warning("Streamlit: Informations sur les features non trouv√©es dans les tags du mod√®le. Tentative de fallback...")

        if all_training_features_str:
            all_training_features = json.loads(all_training_features_str)
            logger.info(f"Streamlit: Liste compl√®te des features d'entra√Ænement r√©cup√©r√©e des tags du mod√®le ({len(all_training_features)} features).")
        else:
            logger.warning("Streamlit: Liste compl√®te des features d'entra√Ænement non trouv√©e dans les tags du mod√®le. Tentative de fallback...")

        # Fallback aux param√®tres de la run si les tags ne sont pas pr√©sents
        if features_info is None or optimal_threshold is None or all_training_features is None:
            logger.warning("Streamlit: Tentative de r√©cup√©ration des m√©tadonn√©es manquantes depuis les param√®tres de la run MLflow...")
            run_info = client.get_run(latest_version.run_id)
            run_info_params = run_info.data.params

            if optimal_threshold is None and 'optimal_threshold_value' in run_info_params:
                optimal_threshold = float(run_info_params['optimal_threshold_value'])
                logger.info(f"Streamlit: Seuil optimal r√©cup√©r√© des param√®tres de la run: {optimal_threshold:.4f}")

            if features_info is None and 'features_info_for_streamlit_json' in run_info_params:
                features_info_json = run_info_params['features_info_for_streamlit_json']
                features_info = json.loads(features_info_json)
                logger.info(f"Streamlit: Informations sur les features r√©cup√©r√©es des param√®tres de la run ({len(features_info)} features).")

            if all_training_features is None and 'all_training_features_names' in run_info_params:
                all_training_features_json = run_info_params['all_training_features_names']
                all_training_features = json.loads(all_training_features_json)
                logger.info(f"Streamlit: Liste compl√®te des features d'entra√Ænement r√©cup√©r√©e des param√®tres de la run ({len(all_training_features)} features).")

        if features_info is None or not features_info:
            raise ValueError("Impossible de d√©terminer la liste des variables d'entr√©e importantes pour l'interface. "
                             "Veuillez vous assurer qu'elles sont loggu√©es correctement lors de l'entra√Ænement.")

        if optimal_threshold is None:
            optimal_threshold = 0.5
            logger.warning(f"Streamlit: Seuil optimal non trouv√© du tout. Utilisation d'un seuil par d√©faut de {optimal_threshold}.")

        if all_training_features is None or not all_training_features:
            logger.warning("Liste compl√®te des features d'entra√Ænement ('all_training_features') non trouv√©e. "
                            "Cela pourrait causer des probl√®mes si les features d'entr√©e ne sont pas ordonn√©es comme pr√©vu.")
            all_training_features = list(features_info.keys()) # Fallback: utiliser les features importantes

        return model, features_info, optimal_threshold, all_training_features

    except Exception as e:
        st.error(f"Streamlit: √âchec critique lors du chargement du mod√®le ou de ses m√©tadonn√©es: {e}")
        st.info("""
            **V√©rifiez les points suivants pour r√©soudre le probl√®me :**
            1.  **Serveur MLflow UI d√©marr√© :** Assurez-vous que `mlflow ui` (avec le bon `--backend-store-uri` si n√©cessaire) est bien en cours d'ex√©cution dans un terminal s√©par√©.
            2.  **Mod√®le enregistr√© :** V√©rifiez que le mod√®le nomm√© `HomeCreditLogisticRegressionPipeline` est visible dans l'onglet "Models" de l'interface MLflow UI et qu'il a des **tags/m√©tadonn√©es** pour 'optimal_threshold' et 'features_info_for_streamlit'.
            3.  **Logs d'entra√Ænement :** Examinez les logs de votre script d'entra√Ænement pour confirmer que le mod√®le a √©t√© enregistr√© sans erreur et que la liste des features et le seuil optimal ont √©t√© loggu√©s correctement.
        """)
        st.stop()
        return None, None, None, None

# --- 5. Chargement des Ressources au D√©marrage de l'Application Streamlit ---
model, features_info, optimal_threshold, all_training_features = load_mlflow_model_and_metadata()


# --- Nouvelle fonction pour afficher le rapport Evidently (lecture seule) ---
# Pas besoin de st.cache_data ici car le fichier est cens√© √™tre g√©n√©r√© √† l'ext√©rieur
def display_evidently_report(report_file_path):
    """
    Charge et affiche le rapport Evidently HTML depuis un fichier.
    """
    if not os.path.exists(report_file_path):
        st.warning(f"Le rapport Evidently n'a pas encore √©t√© g√©n√©r√©. Veuillez d'abord l'ex√©cuter via Docker.")
        st.info(f"Pour g√©n√©rer le rapport, ouvrez un terminal dans le dossier `{os.path.dirname(report_file_path)}` "
                f"et ex√©cutez les commandes Docker comme expliqu√© ci-dessous.")
        return

    try:
        with open(report_file_path, 'r', encoding='utf-8') as f:
            html_content = f.read()
        components.html(html_content, height=1000, scrolling=True)
        st.success(f"Rapport Evidently charg√© et affich√© avec succ√®s depuis : `{report_file_path}`")
    except Exception as e:
        st.error(f"Erreur lors de l'affichage du rapport Evidently : {e}")
        logger.exception("Erreur lors de l'affichage du rapport Evidently dans Streamlit:")


# --- Chemin o√π le rapport Evidently sera sauvegard√© et lu ---
# Important : ce chemin doit √™tre accessible par Streamlit localement
# et doit √™tre le m√™me que le chemin de sortie du Docker
EVLIDENTLY_REPORT_PATH = "evidently_data_drift_report.html" # A la racine de votre projet

# --- 6. Contenu Principal de la Page Streamlit ---
st.title("üìä Pr√©diction de D√©faut Client & Surveillance du Mod√®le")

# Cr√©ation des onglets
tab1, tab2 = st.tabs(["Pr√©diction de Pr√™t", "Analyse du Data Drift"])

with tab1:
    st.markdown("""
    Cette application interactive vous permet de simuler une pr√©diction de risque de d√©faut pour un client.
    Entrez les valeurs des caract√©ristiques ci-dessous et le mod√®le calculera la probabilit√© de d√©faut,
    puis classifiera le client comme √† "Risque √âlev√©" ou "Risque Faible" en utilisant le seuil optimal d√©termin√© lors de l'entra√Ænement.
    """)

    # Affichage des informations sur le mod√®le charg√© dans une barre lat√©rale pour le d√©bogage et l'information
    if model and features_info:
        st.sidebar.header("Informations sur le Mod√®le")
        st.sidebar.write(f"**Nom du Mod√®le :** `{MLFLOW_MODEL_NAME}`")
        try:
            # Tente de r√©cup√©rer la version du mod√®le pour l'affichage
            client = mlflow.tracking.MlflowClient()
            current_version_obj = client.get_latest_versions(MLFLOW_MODEL_NAME, stages=['None', 'Production', 'Staging'])
            current_version = current_version_obj[0].version if current_version_obj else "N/A"
            st.sidebar.write(f"**Version du Mod√®le :** `{current_version}`")
        except Exception as e:
            st.sidebar.warning(f"Impossible de r√©cup√©rer la version du mod√®le: {e}")
        st.sidebar.write(f"**Seuil Optimal Utilis√© :** `{optimal_threshold:.4f}`")
        st.sidebar.write(f"**Nombre de Caract√©ristiques Importantes :** `{len(features_info)}`")
        st.sidebar.info("V√©rifiez les logs de votre terminal pour plus de d√©tails sur le chargement.")

    # --- 7. Interface de Saisie des Caract√©ristiques ---
    if model is not None and features_info is not None and all_training_features is not None:
        st.subheader("Saisie des Caract√©ristiques Client Importantes")
        st.markdown("Veuillez remplir les champs ci-dessous. Les valeurs par d√©faut sont celles du dataset (ou 0.0 si non sp√©cifi√©). "
                    "Vous pouvez les modifier pour voir l'impact sur la pr√©diction. "
                    "Les plages de valeurs sont indicatives, bas√©es sur l'entra√Ænement.")

        user_inputs = {}
        num_columns = 2
        cols = st.columns(num_columns)
        col_idx = 0

        for feature_original_name, info in features_info.items():
            display_name = info.get("display_name", feature_original_name)
            min_val_hint = info.get("min_val")
            max_val_hint = info.get("max_val")

            default_value = (float(min_val_hint) + float(max_val_hint)) / 2 if min_val_hint is not None and max_val_hint is not None else 0.0

            with cols[col_idx]:
                st.write(f"**{display_name}**")
                st.caption(f"Plage: [{min_val_hint:.2f} - {max_val_hint:.2f}]" if min_val_hint is not None and max_val_hint is not None else "Plage: N/A")

                user_inputs[feature_original_name] = st.number_input(
                    label=" ", # Le label est mis √† vide car il est d√©j√† affich√© au-dessus
                    value=float(default_value),
                    min_value=float(min_val_hint) if min_val_hint is not None else None,
                    max_value=float(max_val_hint) if max_val_hint is not None else None,
                    format="%.4f",
                    key=f"input_{feature_original_name}" # Cl√© unique pour chaque widget
                )
            col_idx = (col_idx + 1) % num_columns

        st.markdown("---")

        # --- 8. Bouton de Pr√©diction et Affichage des R√©sultats ---
        if st.button("Obtenir la Pr√©diction", help="Cliquez pour ex√©cuter le mod√®le avec les valeurs saisies."):
            model_input_data = {}
            for feature_name in all_training_features:
                if feature_name in user_inputs:
                    model_input_data[feature_name] = user_inputs[feature_name]
                else:
                    # G√©rer les features non-importantes ou manquantes en leur donnant une valeur par d√©faut
                    model_input_data[feature_name] = 0.0

            input_df = pd.DataFrame([model_input_data])
            input_df = input_df[all_training_features]

            try:
                prediction_proba = model.predict_proba(input_df)[:, 1][0]
                prediction_class = 1 if prediction_proba >= optimal_threshold else 0

                st.subheader("üéâ R√©sultat de la Pr√©diction :")

                col_proba, col_class = st.columns(2)
                with col_proba:
                    st.metric(label="Probabilit√© de D√©faut", value=f"{prediction_proba:.4f}")

                with col_class:
                    if prediction_class == 1:
                        st.error("‚ö†Ô∏è **Client √† Risque de D√©faut √âlev√©**")
                    else:
                        st.success("‚úÖ **Client √† Risque de D√©faut Faible**")

                st.markdown("---")
                st.write(f"**D√©tails de l'analyse :**")
                st.markdown(f"- Le mod√®le a calcul√© une probabilit√© de d√©faut de **`{prediction_proba:.4f}`**.")
                st.markdown(f"- Le **seuil de classification utilis√© est de `{optimal_threshold:.4f}`**. Ce seuil a √©t√© sp√©cifiquement optimis√© lors de l'entra√Ænement pour maximiser la m√©trique F-beta.")
                st.markdown(f"- Si la probabilit√© calcul√©e (`{prediction_proba:.4f}`) est sup√©rieure ou √©gale au seuil (`{optimal_threshold:.4f}`), le client est class√© comme 'Risque √âlev√©'.")

            except Exception as e:
                st.error(f"Une erreur est survenue lors de l'ex√©cution de la pr√©diction : {e}")
                st.warning("V√©rifiez que toutes les valeurs saisies sont num√©riques et coh√©rentes avec les attentes du mod√®le.")
                logger.exception("Erreur lors de la pr√©diction Streamlit:")

    else:
        st.error("L'application n'a pas pu √™tre initialis√©e car le mod√®le ou ses m√©tadonn√©es n'ont pas √©t√© charg√©s. Veuillez r√©soudre les erreurs signal√©es ci-dessus et relancer l'application.")

with tab2:
    st.header("Analyse du Data Drift (Evidently AI)")
    st.markdown("""
    Cette section affiche un rapport interactif g√©n√©r√© par [Evidently AI](https://evidentlyai.com/)
    pour surveiller le **Data Drift** (d√©rive des donn√©es) entre vos donn√©es de r√©f√©rence (entra√Ænement)
    et vos donn√©es actuelles (production). Un drift significatif peut indiquer que votre mod√®le
    doit √™tre r√©entra√Æn√©.

    **Ce rapport est g√©n√©r√© en dehors de l'application Streamlit via Docker.**
    """)
    # Appel de la fonction pour afficher le rapport Evidently
    # Le chemin du fichier est pass√© pour que la fonction puisse le charger.
    display_evidently_report(EVLIDENTLY_REPORT_PATH)

    st.markdown("---")
    st.subheader("Comment g√©n√©rer le rapport de Data Drift :")
    st.info(f"""
    1.  **Assurez-vous que Docker Desktop est install√© et en cours d'ex√©cution.**
    2.  **Ouvrez un terminal ou invite de commande** et naviguez jusqu'au dossier `monitoring` de votre projet :
        `cd C:\\Users\\jonjo\\Documents\\open classrooms\\Projet 7\\monitoring`
    3.  **Construisez l'image Docker** (une seule fois ou apr√®s modification du Dockerfile/requirements.txt) :
        `docker build -t evidently-report-generator .`
    4.  **Ex√©cutez le conteneur Docker** pour g√©n√©rer le rapport HTML :
        `docker run --rm -v "C:\\Users\\jonjo\\Documents\\open classrooms\\Projet 7:/app" evidently-report-generator`

    Ce dernier commande va :
    * `docker run`: Ex√©cuter un conteneur.
    * `--rm`: Supprimer le conteneur apr√®s son ex√©cution (nettoyage automatique).
    * `-v "C:\\Users\\jonjo\\Documents\\open classrooms\\Projet 7:/app"`: **Monter le volume.** C'est crucial ! Cela lie votre dossier de projet local (l√† o√π se trouve `app.py` et o√π vous voulez le `evidently_data_drift_report.html`) au dossier `/app` √† l'int√©rieur du conteneur. Ainsi, le rapport g√©n√©r√© par `generate_report.py` dans `/app/evidently_data_drift_report.html` sera directement √©crit dans votre dossier `C:\\Users\\jonjo\\Documents\\open classrooms\\Projet 7` sur votre machine.
    * `evidently-report-generator`: Le nom de l'image Docker que nous avons construite.

    Apr√®s avoir ex√©cut√© la commande `docker run`, le fichier `evidently_data_drift_report.html` devrait appara√Ætre ou √™tre mis √† jour √† la racine de votre dossier de projet (l√† o√π se trouve `app.py`). Vous pourrez alors actualiser votre page Streamlit pour voir le rapport.
    """)